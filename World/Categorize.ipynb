{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Combine_csv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "\n",
    "Today_time = now.strftime(\"%H:%M\")\n",
    "\n",
    "Today_date = now.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dir =r\"C:\\Users\\drago\\Documents\\GitHub\\Summer-Project\\World\\Combined CSV\\ \"\n",
    "suffix_dir = 'combined_world-'+Today_date+'.csv'\n",
    "today_csv=prefix_dir[:-1] +suffix_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\drago\\\\Documents\\\\GitHub\\\\Summer-Project\\\\World\\\\Combined CSV\\\\combined_world-2019-06-12.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = pd.read_csv(today_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Group: Many breakfast cereals still contaminat...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/12/health/glyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>America's renewable energy capacity is now gre...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/11/business/re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Black drivers in Missouri are 91% more likely ...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/11/us/missouri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Estranged husband pleads not guilty in connect...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/11/us/jennifer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>The SkyDeck ledge of the Willis Tower cracks u...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/12/us/willis-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source                                            Heading Category  \\\n",
       "0    CNN  Group: Many breakfast cereals still contaminat...       us   \n",
       "1    CNN  America's renewable energy capacity is now gre...       us   \n",
       "2    CNN  Black drivers in Missouri are 91% more likely ...       us   \n",
       "3    CNN  Estranged husband pleads not guilty in connect...       us   \n",
       "4    CNN  The SkyDeck ledge of the Willis Tower cracks u...       us   \n",
       "\n",
       "         Date   Time                                                URL  \n",
       "0  2019-06-12  23:53  https://edition.cnn.com/2019/06/12/health/glyp...  \n",
       "1  2019-06-12  23:53  https://edition.cnn.com/2019/06/11/business/re...  \n",
       "2  2019-06-12  23:53  https://edition.cnn.com/2019/06/11/us/missouri...  \n",
       "3  2019-06-12  23:53  https://edition.cnn.com/2019/06/11/us/jennifer...  \n",
       "4  2019-06-12  23:53  https://edition.cnn.com/2019/06/12/us/willis-t...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=technology[technology['Date']==Today_date]\n",
    "df=world\n",
    "\n",
    "Heading = df['Heading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Group: Many breakfast cereals still contaminat...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/12/health/glyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>America's renewable energy capacity is now gre...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/11/business/re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Black drivers in Missouri are 91% more likely ...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/11/us/missouri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Estranged husband pleads not guilty in connect...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/11/us/jennifer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>The SkyDeck ledge of the Willis Tower cracks u...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>23:53</td>\n",
       "      <td>https://edition.cnn.com/2019/06/12/us/willis-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source                                            Heading Category  \\\n",
       "0    CNN  Group: Many breakfast cereals still contaminat...       us   \n",
       "1    CNN  America's renewable energy capacity is now gre...       us   \n",
       "2    CNN  Black drivers in Missouri are 91% more likely ...       us   \n",
       "3    CNN  Estranged husband pleads not guilty in connect...       us   \n",
       "4    CNN  The SkyDeck ledge of the Willis Tower cracks u...       us   \n",
       "\n",
       "         Date   Time                                                URL  \n",
       "0  2019-06-12  23:53  https://edition.cnn.com/2019/06/12/health/glyp...  \n",
       "1  2019-06-12  23:53  https://edition.cnn.com/2019/06/11/business/re...  \n",
       "2  2019-06-12  23:53  https://edition.cnn.com/2019/06/11/us/missouri...  \n",
       "3  2019-06-12  23:53  https://edition.cnn.com/2019/06/11/us/jennifer...  \n",
       "4  2019-06-12  23:53  https://edition.cnn.com/2019/06/12/us/willis-t...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "us                160\n",
       "world              93\n",
       "europe             88\n",
       "china              80\n",
       "uk                 80\n",
       "middle east        66\n",
       "south asia         50\n",
       "pakistan           50\n",
       "rest of world      50\n",
       "mad, mad world     50\n",
       "africa             46\n",
       "asia               44\n",
       "australia          38\n",
       "india              35\n",
       "americas           24\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\drago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Delete This\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "#categories = ['rec.sport.baseball']\n",
    "#dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "#corpus=dataset.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Heading'] = df['Heading'].apply(remove_apostrophe)\n",
    "df['Heading'] = df['Heading'].apply(remove_punctuation)\n",
    "df['Heading'] = df['Heading'].apply(convert_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\drago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "   \n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "   \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heading_stemmed = []\n",
    "Heading_tokenized = []\n",
    "for i in Heading:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'Heading', tokenize/stem\n",
    "    Heading_stemmed.extend(allwords_stemmed) #extend the 'Heading_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    Heading_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group',\n",
       " 'many',\n",
       " 'breakfast',\n",
       " 'cereals',\n",
       " 'still',\n",
       " 'contaminated',\n",
       " 'by',\n",
       " 'weed',\n",
       " 'killer',\n",
       " 'americas']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Heading_tokenized[:10]    # 10 out of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group',\n",
       " 'mani',\n",
       " 'breakfast',\n",
       " 'cereal',\n",
       " 'still',\n",
       " 'contamin',\n",
       " 'by',\n",
       " 'weed',\n",
       " 'killer',\n",
       " 'america']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Heading_stemmed[:10]  # 10 out of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frame = pd.DataFrame({'words': Heading_tokenized}, index = Heading_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mani</th>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cereal</th>\n",
       "      <td>cereals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>still</th>\n",
       "      <td>still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contamin</th>\n",
       "      <td>contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weed</th>\n",
       "      <td>weed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killer</th>\n",
       "      <td>killer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>americas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words\n",
       "group             group\n",
       "mani               many\n",
       "breakfast     breakfast\n",
       "cereal          cereals\n",
       "still             still\n",
       "contamin   contaminated\n",
       "by                   by\n",
       "weed               weed\n",
       "killer           killer\n",
       "america        americas"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_frame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting and applying algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_n(tfidf_matrix,dist):\n",
    "    \n",
    "    n_clusters = list (range (14,20))\n",
    "    min=999\n",
    "    for n in n_clusters:\n",
    "        km_ss = KMeans(n_clusters=n)\n",
    "        clusters_ss = km_ss.fit_predict(tfidf_matrix)\n",
    "        score = silhouette_score(dist,clusters_ss)\n",
    "        if min>score:\n",
    "            min=score\n",
    "            n_score=n\n",
    "    return n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict={}\n",
    "for cat in df['Category'].unique():\n",
    "    temp = df[df['Category']==cat].reset_index().drop(['index'],axis=1)\n",
    "    Heading = temp['Heading']\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,4))\n",
    "    tfidf_matrix = vectorizer.fit_transform(Heading)\n",
    "    \n",
    "    dist = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    n = get_best_n(tfidf_matrix,dist)\n",
    "    km = KMeans(n_clusters=n)\n",
    "    km.fit(tfidf_matrix)\n",
    "    \n",
    "    clusters = km.labels_.tolist()\n",
    "    temp['Cluster'] = clusters\n",
    " \n",
    "    df_sorted=temp.sort_values(by='Cluster').reset_index()\n",
    "    df_sorted.drop(['index'],axis=1,inplace=True)\n",
    "    \n",
    "    grp = df_sorted.sort_values('Cluster').groupby(['Cluster'],as_index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster_similarity_value =[]\n",
    "    \n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    for i in range(n):\n",
    "        group = grp.get_group(i)\n",
    "\n",
    "        cluster_heading=group['Heading']\n",
    "        cluster_matrix = vectorizer.fit_transform(cluster_heading)\n",
    "        cluster_dist = cosine_similarity(cluster_matrix)\n",
    "        cluster_elements_count = pd.DataFrame.count(group)\n",
    "        x=[]\n",
    "        for i in cluster_dist:\n",
    "\n",
    "            if((cluster_elements_count[0]-1)==0):\n",
    "                y=1\n",
    "            else:\n",
    "                y=float(\"{0:.2f}\".format(((i.sum())/(cluster_elements_count[0]))))\n",
    "            x.append(y)\n",
    "            cluster_similarity_value.append(y)\n",
    "   \n",
    "    \n",
    "    \n",
    "    df_sorted['cluster_similarity_value']=cluster_similarity_value;  col=df_sorted.columns\n",
    "    \n",
    "    grp = df_sorted.sort_values('Cluster').groupby(['Cluster'],as_index=False)\n",
    "    \n",
    "    temp_more =[]\n",
    "    temp_less  =[]\n",
    "    for i in range(n):\n",
    "        cluster = grp.get_group(i)\n",
    "        cluster_mean = cluster['cluster_similarity_value'].mean()\n",
    "        cluster_std = cluster['cluster_similarity_value'].std()\n",
    "        comp_fact = cluster_mean + cluster_std/4\n",
    "        for i in range(len(cluster)):\n",
    "            if (cluster.iloc[i]['cluster_similarity_value']<comp_fact):\n",
    "                temp_less.append(cluster.iloc[i])\n",
    "            else:\n",
    "                temp_more.append(cluster.iloc[i])\n",
    "    df_more_similar=pd.DataFrame(temp_more,columns=col)\n",
    "    df_less_similar=pd.DataFrame(temp_less,columns=col)\n",
    "    \n",
    "    \n",
    "    Result = df_more_similar.sort_values('Cluster').groupby(['Cluster'],as_index=False).apply(lambda x: x.sample(1)) \n",
    "    Result = Result.reset_index().drop(['level_0','level_1'],axis=1)\n",
    "    Result = Result.append(df_less_similar)\n",
    "    Result = Result.sort_values(by='Cluster')\n",
    "    Result = Result.reset_index().drop(['index'],axis=1)\n",
    "    \n",
    "    \n",
    "    df_dict[cat] = Result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for typ,data in df_dict.items():\n",
    "    \n",
    "    outname =typ +'.csv'\n",
    "    root = 'Categorized Output/'\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "    date_today= Today_date +'/'\n",
    "    outdir=root+ date_today[:-1]\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    \n",
    "    df.to_csv(fullname,index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
