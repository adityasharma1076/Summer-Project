{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Combine_csv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "\n",
    "Today_time = now.strftime(\"%H:%M\")\n",
    "\n",
    "Today_date = now.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dir =r\"C:\\Users\\drago\\Documents\\GitHub\\Summer-Project\\World\\Combined CSV\\ \"\n",
    "suffix_dir = 'combined_world-'+Today_date+'.csv'\n",
    "today_csv=prefix_dir[:-1] +suffix_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\drago\\\\Documents\\\\GitHub\\\\Summer-Project\\\\World\\\\Combined CSV\\\\combined_world-2019-06-17.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = pd.read_csv(today_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>A black Boeing employee found a noose over his...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/boeing-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>4 dead of gunshots in Iowa home</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/iowa-wes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Phoenix mayor apologizes to family who was det...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/phoenix-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Will you take in a dead gray whale?</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/dead-gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Divers collect 1,500 pounds of trash at a Flor...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/divers-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source                                            Heading Category  \\\n",
       "0    CNN  A black Boeing employee found a noose over his...       us   \n",
       "1    CNN                    4 dead of gunshots in Iowa home       us   \n",
       "2    CNN  Phoenix mayor apologizes to family who was det...       us   \n",
       "3    CNN                Will you take in a dead gray whale?       us   \n",
       "4    CNN  Divers collect 1,500 pounds of trash at a Flor...       us   \n",
       "\n",
       "         Date   Time                                                URL  \n",
       "0  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/boeing-n...  \n",
       "1  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/iowa-wes...  \n",
       "2  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/phoenix-...  \n",
       "3  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/dead-gra...  \n",
       "4  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/divers-l...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=technology[technology['Date']==Today_date]\n",
    "df=world\n",
    "\n",
    "Heading = df['Heading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>A black Boeing employee found a noose over his...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/boeing-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>4 dead of gunshots in Iowa home</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/iowa-wes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Phoenix mayor apologizes to family who was det...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/phoenix-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Will you take in a dead gray whale?</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/dead-gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Divers collect 1,500 pounds of trash at a Flor...</td>\n",
       "      <td>us</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>15:00</td>\n",
       "      <td>https://edition.cnn.com/2019/06/16/us/divers-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source                                            Heading Category  \\\n",
       "0    CNN  A black Boeing employee found a noose over his...       us   \n",
       "1    CNN                    4 dead of gunshots in Iowa home       us   \n",
       "2    CNN  Phoenix mayor apologizes to family who was det...       us   \n",
       "3    CNN                Will you take in a dead gray whale?       us   \n",
       "4    CNN  Divers collect 1,500 pounds of trash at a Flor...       us   \n",
       "\n",
       "         Date   Time                                                URL  \n",
       "0  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/boeing-n...  \n",
       "1  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/iowa-wes...  \n",
       "2  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/phoenix-...  \n",
       "3  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/dead-gra...  \n",
       "4  2019-06-17  15:00  https://edition.cnn.com/2019/06/16/us/divers-l...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "us                162\n",
       "world              93\n",
       "europe             88\n",
       "china              80\n",
       "uk                 80\n",
       "middle east        66\n",
       "rest of world      50\n",
       "south asia         50\n",
       "mad, mad world     50\n",
       "pakistan           50\n",
       "africa             47\n",
       "asia               44\n",
       "australia          38\n",
       "india              35\n",
       "americas           24\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\drago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Delete This\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "#categories = ['rec.sport.baseball']\n",
    "#dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "#corpus=dataset.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Heading'] = df['Heading'].apply(remove_apostrophe)\n",
    "df['Heading'] = df['Heading'].apply(remove_punctuation)\n",
    "df['Heading'] = df['Heading'].apply(convert_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\drago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "   \n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "   \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heading_stemmed = []\n",
    "Heading_tokenized = []\n",
    "for i in Heading:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'Heading', tokenize/stem\n",
    "    Heading_stemmed.extend(allwords_stemmed) #extend the 'Heading_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    Heading_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'black',\n",
       " 'boeing',\n",
       " 'employee',\n",
       " 'found',\n",
       " 'a',\n",
       " 'noose',\n",
       " 'over',\n",
       " 'his',\n",
       " 'desk']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Heading_tokenized[:10]    # 10 out of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'black', 'boe', 'employe', 'found', 'a', 'noos', 'over', 'his', 'desk']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Heading_stemmed[:10]  # 10 out of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frame = pd.DataFrame({'words': Heading_tokenized}, index = Heading_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boe</th>\n",
       "      <td>boeing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employe</th>\n",
       "      <td>employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>found</th>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noos</th>\n",
       "      <td>noose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>over</th>\n",
       "      <td>over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desk</th>\n",
       "      <td>desk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            words\n",
       "a               a\n",
       "black       black\n",
       "boe        boeing\n",
       "employe  employee\n",
       "found       found\n",
       "a               a\n",
       "noos        noose\n",
       "over         over\n",
       "his           his\n",
       "desk         desk"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_frame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting and applying algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_n(tfidf_matrix,dist):\n",
    "    \n",
    "    n_clusters = list (range (14,20))\n",
    "    min=999\n",
    "    for n in n_clusters:\n",
    "        km_ss = KMeans(n_clusters=n)\n",
    "        clusters_ss = km_ss.fit_predict(tfidf_matrix)\n",
    "        score = silhouette_score(dist,clusters_ss)\n",
    "        if min>score:\n",
    "            min=score\n",
    "            n_score=n\n",
    "    return n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict={}\n",
    "for cat in df['Category'].unique():\n",
    "    temp = df[df['Category']==cat].reset_index().drop(['index'],axis=1)\n",
    "    Heading = temp['Heading']\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,4))\n",
    "    tfidf_matrix = vectorizer.fit_transform(Heading)\n",
    "    \n",
    "    dist = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    n = get_best_n(tfidf_matrix,dist)\n",
    "    km = KMeans(n_clusters=n)\n",
    "    km.fit(tfidf_matrix)\n",
    "    \n",
    "    clusters = km.labels_.tolist()\n",
    "    temp['Cluster'] = clusters\n",
    " \n",
    "    df_sorted=temp.sort_values(by='Cluster').reset_index()\n",
    "    df_sorted.drop(['index'],axis=1,inplace=True)\n",
    "    \n",
    "    grp = df_sorted.sort_values('Cluster').groupby(['Cluster'],as_index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster_similarity_value =[]\n",
    "    \n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    for i in range(n):\n",
    "        group = grp.get_group(i)\n",
    "\n",
    "        cluster_heading=group['Heading']\n",
    "        cluster_matrix = vectorizer.fit_transform(cluster_heading)\n",
    "        cluster_dist = cosine_similarity(cluster_matrix)\n",
    "        cluster_elements_count = pd.DataFrame.count(group)\n",
    "        x=[]\n",
    "        for i in cluster_dist:\n",
    "\n",
    "            if((cluster_elements_count[0]-1)==0):\n",
    "                y=1\n",
    "            else:\n",
    "                y=float(\"{0:.2f}\".format(((i.sum())/(cluster_elements_count[0]))))\n",
    "            x.append(y)\n",
    "            cluster_similarity_value.append(y)\n",
    "   \n",
    "    \n",
    "    \n",
    "    df_sorted['cluster_similarity_value']=cluster_similarity_value;  col=df_sorted.columns\n",
    "    \n",
    "    grp = df_sorted.sort_values('Cluster').groupby(['Cluster'],as_index=False)\n",
    "    \n",
    "    temp_more =[]\n",
    "    temp_less  =[]\n",
    "    for i in range(n):\n",
    "        cluster = grp.get_group(i)\n",
    "        cluster_mean = cluster['cluster_similarity_value'].mean()\n",
    "        cluster_std = cluster['cluster_similarity_value'].std()\n",
    "        comp_fact = cluster_mean + cluster_std/4\n",
    "        for i in range(len(cluster)):\n",
    "            if (cluster.iloc[i]['cluster_similarity_value']<comp_fact):\n",
    "                temp_less.append(cluster.iloc[i])\n",
    "            else:\n",
    "                temp_more.append(cluster.iloc[i])\n",
    "    df_more_similar=pd.DataFrame(temp_more,columns=col)\n",
    "    df_less_similar=pd.DataFrame(temp_less,columns=col)\n",
    "    \n",
    "    \n",
    "    Result = df_more_similar.sort_values('Cluster').groupby(['Cluster'],as_index=False).apply(lambda x: x.sample(1)) \n",
    "    Result = Result.reset_index().drop(['level_0','level_1'],axis=1)\n",
    "    Result = Result.append(df_less_similar)\n",
    "    Result = Result.sort_values(by='Cluster')\n",
    "    Result = Result.reset_index().drop(['index'],axis=1)\n",
    "    \n",
    "    \n",
    "    df_dict[cat] = Result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for typ,data in df_dict.items():\n",
    "    \n",
    "    outname =typ +'.csv'\n",
    "    root = 'Categorized Output/'\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "    date_today= Today_date +'/'\n",
    "    outdir=root+ date_today[:-1]\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    \n",
    "    data.to_csv(fullname,index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
